{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <div style=\"color: lightgreen; text-align: center;\">  Funciones AWS Lambda </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"color: lightgreen; text-align: center;\">  _____________________________ </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: lightgreen; text-align: center;\"> .  def developer  . </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import awswrangler as wr\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Obtener el nombre de la desarrolladora del evento\n",
    "    developer_name = event['dev']\n",
    "    \n",
    "    # Conectarse al bucket de S3 donde se encuentra el archivo Parquet\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Descargar el archivo Parquet desde S3\n",
    "    df = wr.s3.read_parquet(\"s3://henry-lab-142024/dev/\", dataset=True)\n",
    "    \n",
    "    df2 = df[df['developer'] == developer_name]\n",
    "    \n",
    "    years_list = sorted(df2['release_date'].dt.year.unique(), reverse=True)\n",
    "    \n",
    "    lista_final = []\n",
    "    \n",
    "    # Creo el bucle para llenar la lista con los diccionarios\n",
    "    for year in years_list:\n",
    "        df_year = df2[df2['release_date'].dt.year == year]\n",
    "        c_items = int(df_year['item_id'].count())  # Contar los items\n",
    "        free = int(df_year[df_year['price'] == 0]['item_id'].count())  # Contar los items con precio 0\n",
    "        m = {str(year): [{'Cantidad de items': c_items}, {'Contenido Free': free}]}\n",
    "        lista_final.append(m)\n",
    "        \n",
    "    result = json.dumps(lista_final)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: lightgreen; text-align: center;\"> .  def best_developer_year  . </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import awswrangler as wr\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Obtener el nombre de la desarrolladora del evento\n",
    "    fecha = int(event['year'])\n",
    "    \n",
    "    # Conectarse al bucket de S3 donde se encuentra el archivo Parquet\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Descargar el archivo Parquet desde S3\n",
    "    df = wr.s3.read_parquet(\"s3://henry-lab-142024/dev/\", dataset=True)\n",
    "    \n",
    "    # Filtro el db con por el año que voy a revisar.\n",
    "    b_dev = df[df['release_date'].dt.year == fecha]\n",
    "    \n",
    "    # Agrupo por la columna developer y sumo los valores de la columna 'positivo'\n",
    "    dev_reviews = b_dev.groupby('developer')['positivo'].sum()\n",
    "    \n",
    "    top_devs = list(dev_reviews.nlargest(3).keys())\n",
    "   \n",
    "   \n",
    "    puesto1 = top_devs[0]\n",
    "    puesto2 = top_devs[1]\n",
    "    puesto3 = top_devs[2]\n",
    "    \n",
    "    top_list = [{\"Puesto 1\": puesto1}, {\"Puesto 2\": puesto2}, {\"Puesto 3\": puesto3}]\n",
    "    \n",
    "    # Convertir la lista a JSON\n",
    "    result = json.dumps(top_list)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: lightgreen; text-align: center;\"> .  def developer_reviews_analysis  . </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    # Obtener el nombre de la desarrolladora del evento\n",
    "    developer_name = event['dev']\n",
    "    \n",
    "    # Conectarse al bucket de S3 donde se encuentra el archivo Parquet\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    \n",
    "    # Descargar el archivo Parquet desde S3\n",
    "    \n",
    "    df = wr.s3.read_parquet(\"s3://henry-lab-142024/dev/\", dataset=True)\n",
    "    \n",
    "    d_data = df[df['developer'] == developer_name]\n",
    "\n",
    "    neg_r = int(d_data['malo'].sum())\n",
    "    pos_r =int( d_data['positivo'].sum())\n",
    "    \n",
    "    format = [{developer_name:{'Negativo': neg_r, 'Positivo': pos_r}}]\n",
    "    \n",
    "    result = json.dumps(format)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: lightgreen; text-align: center;\"> .  def user_data  . </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import awswrangler as wr\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    # Obtener el nombre de usuario del evento\n",
    "    user_v = event['user_1']\n",
    "\n",
    "    # Conectar al bucket de S3 donde se encuentra el archivo Parquet\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Leer el archivo Parquet desde S3 y crear el DataFrame\n",
    "    df = wr.s3.read_parquet(\"s3://henry-lab-142024/user/\", dataset=True)\n",
    "\n",
    "    # Filtrar el DataFrame por el usuario dado\n",
    "    df_user = df[df['user_id'] == user_v]\n",
    "    \n",
    "    # Calcular el dinero gastado y la cantidad de ítems para el usuario dado\n",
    "    money_spent = df_user['price'].sum()\n",
    "    money_spent_rounded = str(round(float(money_spent), 2))\n",
    "    item_count = df_user['item_id'].count()\n",
    "\n",
    "    # Crear el diccionario con la información\n",
    "    result = {\"Usuario\": user_v, \"Dinero_gastado\": money_spent_rounded, \"Cantidad_de_items\": int(item_count)}\n",
    "    \n",
    "    result_j = pd.DataFrame([result]).T\n",
    "    # Convertir el diccionario a formato JSON\n",
    "    final_result = result_j.to_json()\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: lightgreen; text-align: center;\"> .  def UserForGenre  . </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import awswrangler as wr\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    # Obtener el nombre de usuario del evento\n",
    "    user_v = event['user_1']\n",
    "\n",
    "    # Conectar al bucket de S3 donde se encuentra el archivo Parquet\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Leer el archivo Parquet desde S3 y crear el DataFrame\n",
    "    df = wr.s3.read_parquet(\"s3://henry-lab-142024/user/\", dataset=True)\n",
    "\n",
    "    # Filtrar el DataFrame por el usuario dado\n",
    "    df_user = df[df['user_id'] == user_v]\n",
    "    \n",
    "    # Calcular el dinero gastado y la cantidad de ítems para el usuario dado\n",
    "    money_spent = df_user['price'].sum()\n",
    "    money_spent_rounded = str(round(float(money_spent), 2))\n",
    "    item_count = df_user['item_id'].count()\n",
    "\n",
    "    # Crear el diccionario con la información\n",
    "    result = {\"Usuario\": user_v, \"Dinero_gastado\": money_spent_rounded, \"Cantidad_de_items\": int(item_count)}\n",
    "    \n",
    "    result_j = pd.DataFrame([result]).T\n",
    "    # Convertir el diccionario a formato JSON\n",
    "    final_result = result_j.to_json()\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color: lightgreen; text-align: center;\"> .  def recomendacion_juego  . </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos la biblioteca\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la columnas con las que no vamos a trabajar del df\n",
    "df_processed = df.drop(columns=['developer', 'release_date', 'price']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.set_index('item_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49057, 3)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de similitud\n",
    "similarity_matrix = cosine_similarity(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not producto.empty:\n",
    "    product_index = producto.index[0]\n",
    "    product_similarities = similarity_matrix[product_index]\n",
    "    most_similar_products_indices = np.argsort(-product_similarities)\n",
    "    most_similar_products = data.loc[most_similar_products_indices, 'nombre']\n",
    "    print(\"Los productos más similares al producto\", nombre_del_producto, \"son:\")\n",
    "    print(most_similar_products)\n",
    "else:\n",
    "    print(\"Producto no encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 17.9 GiB for an array with shape (1, 49057, 49057) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m similarity_matrix_2 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msimilarity_matrix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 17.9 GiB for an array with shape (1, 49057, 49057) and data type float64"
     ]
    }
   ],
   "source": [
    "if not item_id.empty:\n",
    "    \n",
    "    product_similarities = similarity_matrix[df[df['item_id'] == item_id]]\n",
    "    most_similar_products_indices = np.argsort(-product_similarities)\n",
    "    most_similar_products = data.loc[most_similar_products_indices, 'nombre']\n",
    "    print(\"Los productos más similares al producto\", nombre_del_producto, \"son:\")\n",
    "    print(most_similar_products)\n",
    "else:\n",
    "    print(\"Producto no encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_similar_items(item_id, n=5):\n",
    "    item_index = df_processed.index.get_loc(item_id)\n",
    "    similar_items = similarity_matrix[item_index]\n",
    "    # Obtener los índices de los elementos más similares en orden descendente\n",
    "    similar_items_indices = similar_items.argsort()[::-1][:n]\n",
    "    # Convertir a una lista plana\n",
    "    similar_items_indices = similar_items_indices.flatten()\n",
    "    # Excluir el propio elemento y devolver los elementos similares del DataFrame\n",
    "    return df_processed.iloc[similar_items_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hago una prueba\n",
    "recommended_items = get_top_similar_items(338000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         malo  neutral  positivo\n",
      "item_id                         \n",
      "200210      0        1         0\n",
      "440         1        0         0\n",
      "338000      1        0         0\n",
      "274190      1        0         0\n",
      "369580      0        1         0\n"
     ]
    }
   ],
   "source": [
    "print(recommended_items.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
